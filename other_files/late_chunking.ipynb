{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# # # load model and tokenizer\n",
    "tokenizer2 = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model2 = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "# Load tokenizer and model \n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3' , trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v3' , trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 tokens ['▁Abstract', '▁Ne', 'ural', '▁information', '▁re', 'trie', 'val', '▁(', 'IR', ')', '▁has', '▁great', 'ly', '▁advanced', '▁search', '▁and', '▁other', '▁knowledge', '-', '▁intensiv', 'e', '▁language', '▁task', 's', '.', '▁While', '▁many', '▁ne', 'ural', '▁IR', '▁methods', '▁en', 'code', '▁que', 'ries', '▁and', '▁documents', '▁into', '▁single', '-', 've', 'ctor', '▁representa', 'tions', ',', '▁late', '▁interaction', '▁models', '▁produce', '▁multi', '-', 've', 'ctor', '▁rep', 're', '-', '▁sent', 'ations', '▁at', '▁the', '▁granul', 'ar', 'ity', '▁of', '▁each', '▁to', 'ken', '▁and', '▁de', 'com', 'pose', '▁relevan', 'ce', '▁model', 'ing', '▁into', '▁scal', 'able', '▁to', 'ken', '-', 'level', '▁computa', 'tions', '.', '▁This', '▁de', 'com', 'position', '▁has', '▁been', '▁shown', '▁to', '▁make', '▁late', '▁interaction', '▁more', '▁effective', ',', '▁but', '▁it', '▁infla', 'tes', '▁the', '▁space', '▁foot', 'print', '▁of', '▁these', '▁models', '▁by', '▁an', '▁order', '▁of', '▁magnitud', 'e', '.', '▁In', '▁this', '▁work', ',', '▁we', '▁introduce', '▁Col', 'BER', 'T', 'v', '2', ',', '▁a', '▁re', 'trie', 'ver', '▁that', '▁couple', 's', '▁an', '▁aggressiv', 'e', '▁residu', 'al', '▁compre', 's', '-', '▁', 'sion', '▁mechanism', '▁with', '▁a', '▁de', 'no', 'ised', '▁super', 'vision', '▁strategy', '▁to', '▁simultan', 'e', 'ously', '▁improve', '▁the', '▁quality', '▁and', '▁space', '▁foot', 'print', '▁of', '▁late', '▁interaction', '.', '▁We', '▁evaluat', 'e', '▁Col', 'BER', 'T', 'v', '2', '▁across', '▁a', '▁wide', '▁range', '▁of', '▁benchmark', 's', ',', '▁establish', 'ing', '▁state', '-', 'of', '-', 'the', '-', 'art', '▁quality', '▁within', '▁and', '▁outside', '▁the', '▁training', '▁domain', '▁while', '▁', 'reducing', '▁the', '▁space', '▁foot', 'print', '▁of', '▁late', '▁interaction', '▁models', '▁by', '▁6', '–', '10', '×', '.', '▁When', '▁train', 'ed', '▁on', '▁MS', '▁MAR', 'CO', '▁Passa', 'ge', '▁Rank', '-', '▁ing', ',', '▁Col', 'BER', 'T', 'v', '2', '▁achieve', 's', '▁the', '▁highest', '▁M', 'RR', '@', '10', '▁of', '▁any', '▁standa', 'lone', '▁re', 'trie', 'ver', '.', '▁In', '▁addition', '▁to', '▁in', '-', 'doma', 'in', '▁quality', ',', '▁we', '▁se', 'ek', '▁a', '▁re', 'trie', 'ver', '▁that', '▁general', 'ize', 's', '▁“', 'zero', '-', '▁shot', '”', '▁to', '▁domain', '-', 'specific', '▁corpora', '▁and', '▁long', '-', 'tail', '▁top', '-', '▁', 'ics', ',', '▁ones', '▁that', '▁are', '▁often', '▁under', '-', 're', 'present', 'ed', '▁in', '▁large', '▁public', '▁training', '▁set', 's', '.', '▁To', '▁this', '▁end', ',', '▁we', '▁evaluat', 'e', '▁Col', '-', '▁BER', 'T', 'v', '2', '▁on', '▁a', '▁wide', '▁arra', 'y', '▁of', '▁out', '-', 'of', '-', 'doma', 'in', '▁ben', 'ch', '-', '▁mark', 's', '.', '▁These', '▁include', '▁three', '▁Wikipedia', '▁Open', '-', 'QA', '▁re', 'trie', 'val', '▁tests', '▁and', '▁13', '▁diverse', '▁re', 'trie', 'val', '▁and', '▁sem', 'antic', '-', '▁similar', 'ity', '▁task', 's', '▁from', '▁BE', 'IR', '▁(', 'Tha', 'kur', '▁et', '▁al', '.', ',', '▁2021', ').', '▁In', '▁addition', ',', '▁we', '▁introduce', '▁a', '▁new', '▁benchmark', ',', '▁dub', 'bed', '▁Lo', 'TTE', ',', '▁for', '▁Long', '-', 'Ta', 'il', '▁Topic', '-', 'stra', 't', 'ified', '▁Evalua', 'tion', '▁for', '▁IR', '▁that', '▁features', '▁12', '▁domain', '-', 'specific', '▁search', '▁tests', ',', '▁spanning', '▁Sta', 'ck', 'Ex', 'change', '▁communities', '▁and', '▁using', '▁que', 'ries', '▁from', '▁Goo', 'AQ', '▁(', 'Kha', 's', 'habi', '▁et', '▁al', '.', ',', '▁2021', ').', '▁Lo', 'TTE', '▁focus', 'es', '▁on', '▁relative', 'ly', '▁long', '-', 'tail', '▁topic', 's', '▁in', '▁its', '▁passage', 's', ',', '▁un', 'like', '▁the', '▁Open', '-', 'QA', '▁tests', '▁and', '▁many', '▁of', '▁the', '▁BE', 'IR', '▁task', 's', ',', '▁and', '▁evaluat', 'es', '▁models', '▁on', '▁their', '▁capacity', '▁to', '▁answer', '▁natural', '▁search', '▁que', 'ries', '▁with', '▁a', '▁practical', '▁intent', ',', '▁un', 'like', '▁many', '▁of', '▁BE', 'IR', '’', 's', '▁sem', 'antic', '-', '▁similar', 'ity', '▁task', 's', '.', '▁On', '▁22', '▁of', '▁28', '▁out', '-', 'of', '-', 'doma', 'in', '▁tests', ',', '▁Col', 'BER', 'T', 'v', '2', '▁achieve', 's', '▁the', '▁highest', '▁quality', ',', '▁out', 'per', '-', '▁form', 'ing', '▁the', '▁next', '▁best', '▁re', 'trie', 'ver', '▁by', '▁up', '▁to', '▁8%', '▁relative', '▁gain', ',', '▁while', '▁using', '▁its', '▁com', 'press', 'ed', '▁representa', 'tions', '.']\n",
      "480 tokens2 ['abstract', 'neural', 'information', 'retrieval', '(', 'ir', ')', 'has', 'greatly', 'advanced', 'search', 'and', 'other', 'knowledge', '-', 'intensive', 'language', 'tasks', '.', 'while', 'many', 'neural', 'ir', 'methods', 'en', '##code', 'que', '##ries', 'and', 'documents', 'into', 'single', '-', 'vector', 'representations', ',', 'late', 'interaction', 'models', 'produce', 'multi', '-', 'vector', 'rep', '##re', '-', 'sent', '##ations', 'at', 'the', 'gran', '##ular', '##ity', 'of', 'each', 'token', 'and', 'deco', '##mp', '##ose', 'relevance', 'modeling', 'into', 'scala', '##ble', 'token', '-', 'level', 'computation', '##s', '.', 'this', 'decomposition', 'has', 'been', 'shown', 'to', 'make', 'late', 'interaction', 'more', 'effective', ',', 'but', 'it', 'in', '##fl', '##ates', 'the', 'space', 'footprint', 'of', 'these', 'models', 'by', 'an', 'order', 'of', 'magnitude', '.', 'in', 'this', 'work', ',', 'we', 'introduce', 'colbert', '##v', '##2', ',', 'a', 'retrieve', '##r', 'that', 'couples', 'an', 'aggressive', 'residual', 'com', '##pres', '-', 'si', '##on', 'mechanism', 'with', 'a', 'den', '##oise', '##d', 'supervision', 'strategy', 'to', 'simultaneously', 'improve', 'the', 'quality', 'and', 'space', 'footprint', 'of', 'late', 'interaction', '.', 'we', 'evaluate', 'colbert', '##v', '##2', 'across', 'a', 'wide', 'range', 'of', 'bench', '##marks', ',', 'establishing', 'state', '-', 'of', '-', 'the', '-', 'art', 'quality', 'within', 'and', 'outside', 'the', 'training', 'domain', 'while', 'reducing', 'the', 'space', 'footprint', 'of', 'late', 'interaction', 'models', 'by', '6', '–', '10', '##×', '.', 'when', 'trained', 'on', 'ms', 'marco', 'passage', 'rank', '-', 'ing', ',', 'colbert', '##v', '##2', 'achieve', '##s', 'the', 'highest', 'mr', '##r', '@', '10', 'of', 'any', 'standalone', 'retrieve', '##r', '.', 'in', 'addition', 'to', 'in', '-', 'domain', 'quality', ',', 'we', 'seek', 'a', 'retrieve', '##r', 'that', 'general', '##izes', '“', 'zero', '-', 'shot', '”', 'to', 'domain', '-', 'specific', 'corp', '##ora', 'and', 'long', '-', 'tail', 'top', '-', 'ic', '##s', ',', 'ones', 'that', 'are', 'often', 'under', '-', 'represented', 'in', 'large', 'public', 'training', 'sets', '.', 'to', 'this', 'end', ',', 'we', 'evaluate', 'col', '-', 'bert', '##v', '##2', 'on', 'a', 'wide', 'array', 'of', 'out', '-', 'of', '-', 'domain', 'bench', '-', 'marks', '.', 'these', 'include', 'three', 'wikipedia', 'open', '-', 'q', '##a', 'retrieval', 'tests', 'and', '13', 'diverse', 'retrieval', 'and', 'semantic', '-', 'similarity', 'tasks', 'from', 'bei', '##r', '(', 'tha', '##kur', 'et', 'al', '.', ',', '2021', ')', '.', 'in', 'addition', ',', 'we', 'introduce', 'a', 'new', 'bench', '##mark', ',', 'dubbed', 'lot', '##te', ',', 'for', 'long', '-', 'tail', 'topic', '-', 'st', '##rat', '##ified', 'evaluation', 'for', 'ir', 'that', 'features', '12', 'domain', '-', 'specific', 'search', 'tests', ',', 'spanning', 'stack', '##ex', '##chang', '##e', 'communities', 'and', 'using', 'que', '##ries', 'from', 'goo', '##aq', '(', 'k', '##has', '##hab', '##i', 'et', 'al', '.', ',', '2021', ')', '.', 'lot', '##te', 'focuses', 'on', 'relatively', 'long', '-', 'tail', 'topics', 'in', 'its', 'passages', ',', 'unlike', 'the', 'open', '-', 'q', '##a', 'tests', 'and', 'many', 'of', 'the', 'bei', '##r', 'tasks', ',', 'and', 'evaluate', '##s', 'models', 'on', 'their', 'capacity', 'to', 'answer', 'natural', 'search', 'que', '##ries', 'with', 'a', 'practical', 'intent', ',', 'unlike', 'many', 'of', 'bei', '##r', '’', 's', 'semantic', '-', 'similarity', 'tasks', '.', 'on', '22', 'of', '28', 'out', '-', 'of', '-', 'domain', 'tests', ',', 'colbert', '##v', '##2', 'achieve', '##s', 'the', 'highest', 'quality', ',', 'out', '##per', '-', 'forming', 'the', 'next', 'best', 'retrieve', '##r', 'by', 'up', 'to', '8', '%', 'relative', 'gain', ',', 'while', 'using', 'its', 'compressed', 'representations', '.']\n"
     ]
    }
   ],
   "source": [
    "some_input_text = \"\"\"\n",
    "Abstract\n",
    "Neural information retrieval (IR) has greatly advanced search and other knowledge- intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector repre- sentations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compres- sion mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6–10×.\n",
    "When trained on MS MARCO Passage Rank- ing, ColBERTv2 achieves the highest MRR@10 of any standalone retriever. In addition to in-domain quality, we seek a retriever that generalizes “zero- shot” to domain-specific corpora and long-tail top- ics, ones that are often under-represented in large public training sets. To this end, we evaluate Col- BERTv2 on a wide array of out-of-domain bench- marks. These include three Wikipedia Open-QA retrieval tests and 13 diverse retrieval and semantic- similarity tasks from BEIR (Thakur et al., 2021). In addition, we introduce a new benchmark, dubbed LoTTE, for Long-Tail Topic-stratified Evaluation for IR that features 12 domain-specific search tests, spanning StackExchange communities and using queries from GooAQ (Khashabi et al., 2021). LoTTE focuses on relatively long-tail topics in its passages, unlike the Open-QA tests and many of the BEIR tasks, and evaluates models on their capacity to answer natural search queries with a practical intent, unlike many of BEIR’s semantic- similarity tasks. On 22 of 28 out-of-domain tests, ColBERTv2 achieves the highest quality, outper- forming the next best retriever by up to 8% relative gain, while using its compressed representations.\n",
    "\"\"\"\n",
    "tokens = tokenizer.tokenize(some_input_text)\n",
    "print(len(tokens) , \"tokens\" , tokens)\n",
    "tokens2 = tokenizer2.tokenize(some_input_text)\n",
    "print(len(tokens2) , \"tokens2\" , tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks 2 ['Berlin is the capital and largest city of Germany, both by area and by population.', \" \\nIts more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_new(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.7929044\n",
      "similarity_trad(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.8531231\n",
      "similarity_new(\"Berlin\", \" \n",
      "Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.7890661\n",
      "similarity_trad(\"Berlin\", \" \n",
      "Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.56780535\n"
     ]
    }
   ],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations\n",
    "\n",
    "def chunk_by_sentences_v3(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer, compatible with Jina v3's tokenization\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    token_offsets = inputs[\"offset_mapping\"][0]\n",
    "    token_ids = inputs[\"input_ids\"][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Find all potential sentence endings (periods)\n",
    "    chunk_positions = []\n",
    "    \n",
    "    for i, (token_id, token) in enumerate(zip(token_ids, tokens)):\n",
    "        # Skip if this isn't the last token\n",
    "        if i >= len(token_ids) - 1:\n",
    "            continue\n",
    "            \n",
    "        # Check if this token ends with a period\n",
    "        if token.endswith('.'):\n",
    "            next_token = tokens[i + 1]\n",
    "            # Check if next token starts with ▁ (indicating new word)\n",
    "            if next_token.startswith('▁'):\n",
    "                position = (i, int(token_offsets[i][1]))\n",
    "                chunk_positions.append(position)\n",
    "    \n",
    "    # Create chunks and span annotations\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    \n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    \n",
    "    return chunks, span_annotations\n",
    "\n",
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "input_text = \"\"\"Berlin is the capital and largest city of Germany, both by area and by population. \n",
    "Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. \n",
    "The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\"\"\n",
    "\n",
    "# determine chunks\n",
    "chunks, span_annotations = chunk_by_sentences_v3(input_text, tokenizer)\n",
    "print(\"chunks\" , len(chunks) , chunks)\n",
    "# print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')\n",
    "\n",
    "# chunk before\n",
    "embeddings_traditional_chunking = model.encode(chunks)\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = late_chunking(model_output, [span_annotations])[0]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "berlin_embedding = model.encode('Berlin')\n",
    "\n",
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 35378, 8999, 38, 1129, 2439, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 11), (11, 12), (13, 15), (15, 18), (0, 0)]}\n",
      "{'input_ids': [0, 35378, 8999, 38, 1129, 2439, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7592, 2088, 999, 18414, 11733, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 11), (11, 12), (13, 15), (15, 18), (0, 0)]}\n",
      "(0, 0)\n",
      "0\n",
      "(0, 0)\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple text\n",
    "text = \"Hello world! juila\"\n",
    "encoded = tokenizer(text, return_offsets_mapping=True)\n",
    "print(encoded)\n",
    "encoded2 = tokenizer(text, return_offsets_mapping=False)\n",
    "print(encoded2 )\n",
    "encoded2 = tokenizer2(text, return_offsets_mapping=True)\n",
    "print(encoded2 )\n",
    "punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "token_offsets = encoded['offset_mapping'][0]\n",
    "token_ids = encoded['input_ids'][0]\n",
    "print(token_offsets)\n",
    "print(token_ids)\n",
    "token_offsets2 = encoded2['offset_mapping'][0]\n",
    "token_ids2 = encoded2['input_ids'][0]\n",
    "print(token_offsets2)\n",
    "print(token_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    args:\n",
    "      input_text: The text snippet to split into sentences\n",
    "      tokenizer: The tokenizer to use\n",
    "    return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids(\".\")\n",
    "    sep_id = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "    token_offsets = inputs[\"offset_mapping\"][0]\n",
    "    token_ids = inputs[\"input_ids\"][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations\n",
    "\n",
    "\n",
    "def late_chunking(model_output, span_annotation: list, max_length=None):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
